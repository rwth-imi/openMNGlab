{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "HTML(\"\"\"<style>.container { width:100% !important; } div.output_area {overflow-y: scroll;} div.output_area img {max-width: unset;} </style>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "import eli5 \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from feature_extraction import get_adaptive_spike_count, get_spike_count\n",
    "from machine_learning import * \n",
    "from importers import SpikeImporter\n",
    "from feature_extraction import calc_dist_to_prev_reg_el_stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract AP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/\"\n",
    "file_name = '20_05_13_U1a_4960s_5400s'#\"20_05_13_U1a_complete\"\n",
    "file_ending = \".csv\"\n",
    "\n",
    "time_channel = \"Time\"\n",
    "signal_channel = \"1 Signal\"\n",
    "stimulus_channel = \"32 DigMark\"\n",
    "ap_marker_channels = [\"3 nw-1\"]\n",
    "extra_stimuli_channel = \"801 DigMark\"\n",
    "\n",
    "# load the animal file\n",
    "importer = SpikeImporter(filepath = data_folder + file_name + file_ending, time_channel = time_channel, signal_channel = signal_channel)\n",
    "max_gap_time = 0.005\n",
    "el_stimuli = importer.get_electrical_stimuli(regular_stimulus_channel = stimulus_channel)\n",
    "el_extra_stimuli = importer.get_extra_stimuli(extra_stimulus_channel = extra_stimuli_channel, regular_el_stimuli = el_stimuli, verbose = False)\n",
    "actpots = importer.get_action_potentials(ap_marker_channels = ap_marker_channels, verbose = False)\n",
    "raw_signal = importer.get_raw_signal_split_by_stimuli(el_stimuli = el_stimuli, verbose = False)\n",
    "\n",
    "for ap in actpots:\n",
    "    ap.features[\"spikecount\"] = get_adaptive_spike_count(actpot = ap, actpots = actpots, timeframe = 1600, num_splits = 16)\n",
    "    ap.features[\"latency\"], ap.prev_stimuli[\"regular\"] = calc_dist_to_prev_reg_el_stimulus(ap, el_stimuli)\n",
    "    \n",
    "#candidate_aps = [ap for ap in actpots if ap.implied_fibre_index == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(np.array(pd.DataFrame([ap.features['spikecount'] for ap in actpots]).corr()))\n",
    "heatmap.set_title(\"Pearson's correlation coefficient of X variables\")\n",
    "print(f'There are {len(actpots)} action potentials.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe time dependence via correlation graph of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {'Overfitting check': [], 'Accuracy check': [], 'Homoscedascity check': []}\n",
    "model = LinearRegression()\n",
    "overfitting_metric = mean_squared_error\n",
    "accuracy_metric = r2_score\n",
    "test_percentage = 0.25\n",
    "splitting_method = timebased_train_test_split  # Change this to primitive_split to see the difference\n",
    "\n",
    "for i in range(0, 100):\n",
    "    \n",
    "    train_aps, test_aps = splitting_method(actpots, test_percentage = test_percentage, num_test_intervals = 10)\n",
    "    X_train = [ap.features[\"spikecount\"] for ap in train_aps]\n",
    "    y_train = [ap.features[\"latency\"] for ap in train_aps]\n",
    "    X_test = [ap.features[\"spikecount\"] for ap in test_aps]\n",
    "    y_test = [ap.features[\"latency\"] for ap in test_aps]\n",
    "    \n",
    "    fitted_model = model.fit(X = X_train, y = y_train)\n",
    "    eval_dict['Overfitting check'].append(overfit_check(X_train, X_test, y_train, y_test, fitted_model, overfitting_metric))\n",
    "    eval_dict['Accuracy check'].append(accuracy_check(X_train, X_test, y_train, y_test, fitted_model, accuracy_metric))\n",
    "    eval_dict['Homoscedascity check'].append(homoscedascity_check(X_train, X_test, y_train, y_test, fitted_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(eval_dict), figsize=(20, 4))\n",
    "plt.subplots_adjust(bottom=-0.3)\n",
    "plt.suptitle(f\"{type(model)} # of tr. samples: {floor(len(actpots)*(1-test_percentage))}, # of test samples: {floor(len(actpots)*test_percentage)} {splitting_method}\", fontsize=14)\n",
    "\n",
    "for i, column in enumerate(eval_dict):\n",
    "    \n",
    "    if column == 'Homoscedascity check':\n",
    "        reshaped_arr = np.array(eval_dict[column]).reshape(-1, 2)\n",
    "        _ = sns.scatterplot(x=reshaped_arr[:, 0], y=reshaped_arr[:, 1], ax=axs[i])\n",
    "        axs[i].set_title('Residual/Homoscedascity check')\n",
    "        axs[i].set_xlabel('ŷ')\n",
    "        axs[i].set_ylabel('y-ŷ')\n",
    "    elif column == 'Overfitting check':\n",
    "        _ = sns.distplot(eval_dict[column], ax=axs[i], rug=True)\n",
    "        axs[i].set_title(column)\n",
    "        axs[i].set_xlabel('Count')\n",
    "        axs[i].set_ylabel('Train - test results')\n",
    "    else:\n",
    "        _ = sns.distplot(eval_dict[column], ax=axs[i], rug=True)\n",
    "        axs[i].set_title(column)\n",
    "        axs[i].set_xlabel('Accuracy results')\n",
    "        axs[i].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the last fitted model as an example\n",
    "eli5.show_weights(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
